{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-vXIF8DpQIj"
      },
      "source": [
        "# CS541: Applied Machine Learning, Spring 2025, Lab 8\n",
        "\n",
        "Lab 8 is an exercise that explores Convolutional Neural Networks (CNNs). Convolutional Neural Network (CNN) is the extended version of artificial neural networks (ANN) which is predominantly used to extract the feature from the grid-like matrix dataset. For example visual datasets like images or videos where data patterns play an extensive role. They utilize layers like convolutional layers, transposed convolution layers and pooling layers. They can perform tasks such as semantic segmentation, instance segmentation, image classification and detection and to name a few.\n",
        "\n",
        "**Lab Grading**\n",
        "\n",
        "Labs are hands-on exercises designed to provide guided experience in key concepts through this class.  You are graded based on in-lab participation (not correctness), and **are required to submit** your lab work after class, before Friday of that week.  *Make sure you fill out the attendence form before leaving class*.\n",
        "\n",
        "For students who miss a lab, you can submit a make-up lab on gradescope by the Friday directly following the lab for partial credit.  Please see the syllabus for the lab grading policy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "I2fo-Vyv1Ycg",
        "outputId": "e3af5bee-ff97-4af7-c106-1b7290b5be5a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm\n",
        "import time\n",
        "\n",
        "import torchvision.models as models # contains a lot of pretrained models you can use.\n",
        "# https://pytorch.org/vision/stable/models.html\n",
        "\n",
        "from torchvision.models import resnet50, ResNet50_Weights, resnet18, ResNet18_Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFq-2PeDuwGS",
        "outputId": "ac65cb89-13ae-4078-9d86-8e545b5621e8",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Package                 Version\n",
            "----------------------- -----------\n",
            "appnope                 0.1.4\n",
            "asttokens               3.0.0\n",
            "comm                    0.2.2\n",
            "contourpy               1.3.1\n",
            "cycler                  0.12.1\n",
            "debugpy                 1.8.11\n",
            "decorator               5.2.1\n",
            "exceptiongroup          1.2.2\n",
            "executing               2.1.0\n",
            "filelock                3.18.0\n",
            "fonttools               4.56.0\n",
            "fsspec                  2025.3.0\n",
            "importlib_metadata      8.6.1\n",
            "ipykernel               6.29.5\n",
            "ipython                 9.0.2\n",
            "ipython_pygments_lexers 1.1.1\n",
            "jedi                    0.19.2\n",
            "Jinja2                  3.1.6\n",
            "jupyter_client          8.6.3\n",
            "jupyter_core            5.7.2\n",
            "kiwisolver              1.4.8\n",
            "MarkupSafe              3.0.2\n",
            "matplotlib              3.10.1\n",
            "matplotlib-inline       0.1.7\n",
            "mpmath                  1.3.0\n",
            "nest_asyncio            1.6.0\n",
            "networkx                3.4.2\n",
            "numpy                   2.2.4\n",
            "packaging               24.2\n",
            "parso                   0.8.4\n",
            "pexpect                 4.9.0\n",
            "pickleshare             0.7.5\n",
            "pillow                  11.1.0\n",
            "pip                     25.0\n",
            "platformdirs            4.3.7\n",
            "prompt_toolkit          3.0.50\n",
            "psutil                  5.9.0\n",
            "ptyprocess              0.7.0\n",
            "pure_eval               0.2.3\n",
            "Pygments                2.19.1\n",
            "pyparsing               3.2.3\n",
            "python-dateutil         2.9.0.post0\n",
            "pyzmq                   26.2.0\n",
            "setuptools              75.8.0\n",
            "six                     1.17.0\n",
            "stack_data              0.6.3\n",
            "sympy                   1.13.1\n",
            "torch                   2.6.0\n",
            "torchvision             0.21.0\n",
            "tornado                 6.4.2\n",
            "tqdm                    4.67.1\n",
            "traitlets               5.14.3\n",
            "typing_extensions       4.12.2\n",
            "wcwidth                 0.2.13\n",
            "wheel                   0.45.1\n",
            "zipp                    3.21.0\n"
          ]
        }
      ],
      "source": [
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COD1J-Ha76bB",
        "outputId": "8e255a14-3948-42fd-90d4-c947f62e0b5e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "# Load the CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "14w8MpcfAgMa",
        "outputId": "41450376-8fc7-4a9c-81b6-33c756c8d168",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2082it [00:03, 563.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 2.035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4100it [00:07, 561.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  4000] loss: 1.799\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6083it [00:11, 531.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  6000] loss: 1.653\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8074it [00:15, 490.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  8000] loss: 1.593\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10100it [00:19, 593.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 10000] loss: 1.575\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12099it [00:22, 574.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 12000] loss: 1.531\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12500it [00:33, 372.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 47.66%\n",
            "Avg time taken for prediction: 0.0006412122726440429\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define a simple CNN architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 10, 7, padding=\"same\") # inchannel, outchannel, kernel size\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc3 = nn.Linear(2560, 10)  # Adjust the output classes accordingly\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x))) # this is a square output\n",
        "        x = x.view(x.shape[0], -1)  # Adjust the feature map size accordingly\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the network\n",
        "net = SimpleCNN().to(device)\n",
        "\n",
        "# Define the loss function and optimizer, we don't use momentum here\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0)\n",
        "\n",
        "# Training the network\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in tqdm.tqdm(enumerate(trainloader, 0)):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "avg_time = []\n",
        "for data in testloader:\n",
        "    images, labels = data\n",
        "    start = time.time()\n",
        "    outputs = net(images.to(device))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    end = time.time()\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.cpu() == labels).sum().item()\n",
        "    time_taken = end - start\n",
        "    avg_time.append(time_taken)\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total}%\")\n",
        "print(f\"Avg time taken for prediction: {np.average(avg_time)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDR29xzLsbuL"
      },
      "source": [
        "### Here we want to add momentum for our Optimizer and we can we it helps our classification results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rpbW3BBFpGMC",
        "outputId": "b06a544b-56cc-4f68-d843-7dda0530bfe6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2053it [00:03, 667.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 1.798\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4093it [00:06, 725.70it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  4000] loss: 1.591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6126it [00:08, 710.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  6000] loss: 1.486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8070it [00:11, 700.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  8000] loss: 1.412\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10135it [00:14, 721.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 10000] loss: 1.429\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12105it [00:17, 690.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 12000] loss: 1.398\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12500it [00:28, 444.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 55.24%\n",
            "Avg time taken for prediction: 0.0011408693313598632\n"
          ]
        }
      ],
      "source": [
        "# Initialize the network\n",
        "net = SimpleCNN().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training the network\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in tqdm.tqdm(enumerate(trainloader, 0)):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "avg_time = []\n",
        "for data in testloader:\n",
        "    images, labels = data\n",
        "    start = time.time()\n",
        "    outputs = net(images.to(device))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    end = time.time()\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.cpu() == labels).sum().item()\n",
        "    time_taken = end - start\n",
        "    avg_time.append(time_taken)\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total}%\")\n",
        "print(f\"Avg time taken for prediction: {np.average(avg_time)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXGDzrMjCBKK"
      },
      "source": [
        "## Let's visualize what the filters are learning for our task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9Xk22ots0mUx",
        "outputId": "52ce9e4b-a558-4a91-cf31-395f7097821b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "conv1.weight\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXdJREFUeJzlmK2KQmEURbebMVi9TQSNBtEbfAGDQdFqFYNiEZPN6COIz2DyHUQQfAKDFrtZ/Al3mHjmwIxhwGF/qy1Y4cLm8MFNJUmSQABCBEIEQgRCBEIEQgRCBEKEj1fDRqNhPJ/Pu6Zerxs/n8+uORwOxq/Xq2tyuZzx5XIZziKECERoN/J4PIyfTifXdLvdH+/qi36/b/x2u7kmjmMEuwghAiECEdqxDwYD44vFwjW9Xs/4er12TbPZ/LUpFosIdhFCBCK0G4m/PVLVatU1+/3e+Gq1cs1kMjGezWZdczweEewihAiECIQIqVd/mY5GI+OFQsE1mUzGeKVScc1utzO+3W5d0263jY/H43AWIUQgQnsQN5uN8el06ppOp2P8crm4Jp1OG4+iyDXlchnBLkKIQIhAhPYgDodD4/f73TXP59N4q9VyTalUMl6r1Vwzn8+Nz2azcBYhRCBCu5H/DiECIQIhAiECIQIhAiECIQLf/QF/xSf2Jlg96cSTHAAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXxJREFUeJzlmKGKAmEURr/5EBRBEMRkEIM+gBhMotU0L2AyCIJBwzyCTyEYzRab0WawmVWMFi22WTbevaCGhV2+/7QDh5mBy+WfmShN0xQCECIQIhAiECIQIhAiECIQImQ+DQeDgfF8Pu+aOI6NH49H15zPZ+OdTsc1uVzOeL/fD2cihAhEaDsSRZHxx+Phmmw2a3w2m7lmNBq9vO433W4XwU6EEIEQgQht2avVqvHVavX2IKvX666p1WrGN5uNa5rN5st7S0+EEIEQIfr0L8rpdDK+3W5dMxwOjS8WC9e0Wi3jh8PBNff73fhkMglnIoQIhAhEaMse//j6S5LENev12nixWHTN9Xo1XqlUXNNut433er1wJkKIQIT20ni73YzP53PXjMdj45fLxTXP59P4brdzTaPRQLATIUQgRCBCOxCn06nx/X7vmlKpZLxQKLimXC6//WW6XC5fHrTSEyFEIELbkf8OIQIhAiECIQIhAiECIQIhAv/6AX6LL/MyWM3MOLWRAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXZJREFUeJzlmLuKwlAURXc2g2ClTawEEcTCL7L20foJVn6DrY0ofoFtGkEFQYQI1haCiDYWYpNhysMBHYaBGfZd3QqrCBwOuTdRlmUZBCBEIEQgRCBEIEQgRCBEIET4+G44Ho+NbzYb15TLZePn89k1+/3e+G63c0273Tbe7/fDmQghAhHajiwWC+OXy8U1cRwbz+VyrjmdTsar1apr0jRFsBMhRCBEIEJb9larZXw0GrlmOBwaHwwGrimVSsaPx6NrGo0Ggp0IIQIhQvTTvyidTsc9m8/nxmu1mmuazabx7XbrmiRJjB8Oh3AmQohAiECEtuzdbtd4r9dzzWq1Mj6bzVzzfD6N3+9319TrdePT6TSciRAiEKEdGvP5vPHJZOKaKIqMPx4P11yv15d/Vb6oVCoIdiKECIQIRGgfxOVy+XbZi8Wi8dvt5ppCofD2hLxer1/ePKUnQohAhH5D/G8QIhAiECIQIhAiECIQIhAi8K9f4Lf4BMshaarGDaVJAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAW1JREFUeJzlmDGKwmAQRj8/FxVU1MLCAwgqhBSKIHgOr+EVxHPkDJ7AwsbCylastBOSCJ4gW8+OEIuFXb7/dQ9eERyGiakURVFAAEIEQgRCBEIEQgRCBEIEQoSvT8PNZmP8crm4plarGd/v964h7W/X6XRcs1gsjCdJEs5ECBGI0Hbker0aXy6XromiyPh0OnVNv9833mg0XHM+nxHsRAgRCBGI0JZ9NpsZj+PYNa1Wy/h6vXbN8/k03u12XTMcDhHsRAgRiNB2JMsy4/f73TWDwcD4drt1zXw+N346nUpfLKMfh1Z6IoQIhAiECJVPP5mOx2Pj9Xq99Ggej0fXpGlqfLVauebxeBjf7XbhTIQQgQhtR3q9nvF2u+2aarVqPM/z0mP37h/iaDQyfjgcwpkIIQIhAhHa2+/r9TI+mUxKPxk1m83SY/du2W+3G4KdCCECEdpB/O8QIhAiECIQIhAiECIQIhAi8K8f4Lf4BiuGTratm4TCAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAV1JREFUeJzt2TGKwmAUBOBxXBBEISBiZWMpGEhrq54kh7Cz9BiCp7D2DqmtFC0SwcZOsqR8/kuyxcIu8/brBoYQeLz8P6RVlmUJAYQIQgQhghBBiCBEECIIER/fLe73e5PzPA86w+HQ5MPhEHSu16vJ8/k86NxuN5N3u52fiRAiCG878nq9TO73+0Gn2+2avFgsgk6n0zH5fr8HnSRJ4HYihAhCBOFt2Y/Ho8nP5zPobLfbxucURVG7/JXT6QS3EyFEEN52pN1u117+Kuv1uvGwG41GJi+Xy6BD0u9ECBGECMLbsg8GA5OjKGr8ILwfopVer2fyeDwOOpfLxeQ0Tf1MhBBBeNuR1Wpl8mw2CzpxHDd23i+N0+k06ERf7J+biRAiCBGEt2V/PB61t9jKZDIxebPZBJ0sy0w+n8+Nh6ariRAiCBGt///sfwwhghBBiCBEECIIEYQI/vYL/JRP+ItKOY74B6AAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXVJREFUeJzll6GqAlEURfdstGqaYNAg/oJgkRF/wb8w2G1+hNkk+B+KUcOIKGIRBEGwaLHoPF48HngaHrzHvqutYYWBw+WeG2VZlkEAQgRCBEIEQgRCBEIEQgRChNyn4Xa7Nb5arVyTpqnx/X7vmvv9bjyKItfU63Xj/X4/nIkQIhAiRJ8uje1223iSJK6pVCrG8/m8ay6Xi/HT6eSa+XxufDabhTMRQgRCBCK0C/HxeBgfj8eu6Xa7xtfrtWsKhYLxVqvlmlqthmAnQohAhHZGGo2G8eFw6JrBYGC8Wq26plQqGT8ej65pNpsIdiKECIQIRGjbb/ry+ptMJq55/XY4HFxTLBaNx3H89tJcLBbhTIQQgQjtjLxyu93ct81mY3y327nmer0afz6frlkul8ZHoxGCmQghAiECEdr22+v13m6o5XLZeKfTcc35fP7x5fnNdDpFsBMhRCBCvxD/G4QIhAiECIQIhAiECIQIhAj86x/4Lb4AzsZqjj86WocAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAWVJREFUeJzl1zGKwkAYBeDn08JCiARtLBWLGGxsbK09h2fxIB7DI9gIamEaO8FgJyJpJMuWv/9iLBZ2efN1LzySwM8wM7WyLEsIIEQQIggRhAhCBCGCEEGIaHxaXCwWJne7Xdd53Vsfj4frNJtNk+/3u+u0222Tl8tlOBMhRBAiap8eGufzucmTycR16vW6yZfLpfK9h8PBPbvdbibv9/twJkKIIEQQoW2IWZZVboin08nkwWDgP9iwn2y1Wq5zPp8R7EQIEURoayRNU5M3m03lGtlut64TRVHlobHf7yPYiRAiCBFEaIv9er2+Pel+m81mJq/Xa9cZDocmJ0niOrvdDsFOhBBBhHZDXK1WJsdx7Dp5nps8Go0qb38/eT6fb2+n0hMhRBAiiNA2xOjl1DqdTl3neDya3Ov1XGc8HptcFIXrdDodBDsRQgQR2ob43xEiCBGECEIEIYIQQYggRPCvf+C3fAFN7ljHyqnPOQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXZJREFUeJztl62qAlEURr/5uNEkqDAWm1iEaUbBIDajTyL4Aj6DzUewi1GrYBpFxS5o8QeLzM3bXQwX7r3fcbUFi8PAnsOeibIsyyAAIQIhAiECIQIhAiECIQIhwte74XK5ND6fz10Tx7Hxw+Hgmnq9bjyKItfc73fj3W43nIkQIhCh3ZH45f0vlUquaTQaxtM0dc1mszHe6XRcc7vdEOxECBEIEQgRonf/ECeTifFqteoPe1lutVrNNavVyvjxeHTN6XQy3uv1wpkIIQIR2kI8n8/G+/2+a7bbrfHhcOia8XhsvN1uuyZ9WaSfO/IfIUQgQl+IhULBNdPp1PhoNHLN5XIx/ng8XNNqtYzPZrNwJkKIQIS2EJ/Pp/H1eu2aJEmMDwYD1+RyOeP7/d41i8UCwU6EEIEQgQjtsu92O+PNZtM1lUrFeLlcdk2xWDR+vV5dk8/nEexECBGI0D4a/zqECIQIhAiECIQIhAiECIQI/O0H+Cm+Aa/dY85gD4jUAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXRJREFUeJzl2KGKAmEUBeAzhwVBm22KRm0WQfAxLIrR4mP4AmZfwegTCFonKRgMBg0jgkGbiGGWiXcvDC4Iu5z/awdOGOZy//mZKMuyDAIIEYQIQgQhghBBiCBEECK+3i12Oh2TK5WK6zweD5MbjYbr9Ho9k6Mocp3r9WryaDQKZyKECCK0HZlOpyafz2fXud/vJh8OB9d5vV6Fe5UbDAYIdiKECEIEEdqyr1Yrk/v9vuscj0eTW62W69TrdZPjOMYnECIIEURoO3K73Uxer9eu83w+TS6Xy65TrVZNTpLEdS6Xi8nj8TiciRAiCBGEiOjdX6a73c7k5XLpOrPZzGTSv6f9fm9ys9l0nclk8uvbsMxECBFEaDuyWCwKdyZXKpUKL5q5NE1NHg6HrrPZbEyez+fhTIQQQYggQrv9brfbwoXMtdttk7vdruv8PABOp5Pr1Go1BDsRQgQR2gfxvyNEECIIEYQIQgQhghBBiOBfP8CnfAMqyWYR6xBUvwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAWZJREFUeJztmTGrgQEUht/vpaTYpER+gNHkD5gkA6N/YPWzSDFYjCasUrKI0WRh+27G0ynucOve3nOf7aln+OrthE+SpmkKAQgRCBEIEQgRCBEIEQgRCBGy3w2Xy6Xx0+nkmtVqZbzRaLhmNBoZr9VqrplMJsYHg0GcRQgRiGg38nw+jXe7XdckSWJ8PB67ZrfbGZ/P565pNpsIuwghAiECEe3Y9/v9x4OsVCrGW63Wxw/N2Wzmmk6ng7CLECIQ0W4kl8sZn06nrun1esb7/b5rDoeD8fP57JrFYmF8OBzGWYQQgRCBiHbs9Xrd+PF4dM39fjfebrddc71ejW82G9fk83mEXYQQgYh2I5fLxXgmk3HNer02XiqVXFOtVo0XCgXX3G43hF2EEIEQgYh27OVy+eOrzu12+/YX44tisfj22/CLx+OBsIsQIhAiJP//s/8xCBEIEQgRCBEIEQgRCBH42w/wU3wBbUFRINRO06kAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAWlJREFUeJztmaGKQlEURffbiGgQf0GTTRSTxWKx+COC2eC3CP6BBov9FUF/QAwGg8iLWgR5w7Q5HNEJAzPsM6stWOHB4dwL9yV5nucQgBCBEIEQgRCBEIEQgRCBEKHw3XC1WhnfbreuWa/Xxjudjmuazabx4XDomsViYXw0GsWZCCECEW1HDoeD8clk4pr7/W58Pp+7ZrfbGR8MBq7pdrsIOxFCBEIEItqy7/d744/HwzXj8dj45XJxzWw2e3n5fVKv1/GVdruNMBMhRCCi7UixWDS+2Wxc0+/3jfd6vbcX4nK5dM10OkXYiRAiECIQ0ZY9SRLjWZa5Jk1T47Va7e2BcLvdXFMqlRB2IoQIRLQdaTQaxs/ns2uu1+vbV5RKpWL8eDy6Jnuyf2EmQohAiEBEW/Zqtfrygnz2ZPTsQGi1WsbL5bJrTqcTwk6EEIEQIfn/z/7HIEQgRCBEIEQgRCBEIETgb3/AT/EBUN5XiZUBEbAAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAWtJREFUeJztmTFqAmEQRj8/xE5EEBZrD2BjJWyn1oLgAWwEW/EeiqCnsLWwsxRBsLLWM6iVuCHlZEJMEUj4Jq978GAXZoef3c1lWZZBAEIEQgRCBEIEQgRCBEIEQoT8d8PRaPSyWS6XxhuNhmsGg4HxJElcM5vNjG+32zgTIUQgou3I/X433uv1XHO9Xl8+24vFwvhkMnFNu91G2IkQIhAiENGWfbPZGG+1Wq4Zj8fGK5WKa+bzufHD4eCaarWKsBMhRCCi7cjtdjP+eDxcU6/Xjadp6prpdGr8crm4ptlsIuxECBEIEYhoy14sFo2vVivXPJ/Pl2+IHw/S8/nsmkKhgLATIUQgou3IcDg0Xi6XXXM8Ho33+33XdDod4+v12jWn08l4t9uNMxFCBEIEItqy73a7Lw+/d2q1mvH9fu8vmLeXLJVKrkk++YwaZiKECIQIuf//7H8MQgRCBEIEQgRCBEIEQgT+9g38FG/YZFCT/kzpLgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXFJREFUeJztmT1qAmEURa9X8Q97GxdgY2MpiK0LsLFxAYILcAN27sAl2GtlJWplJWipjaONS3BCypcnSYpAwn053YED38DjMTN8mTRNUwhAiECIQIhAiECIQIhAiECIkPtuOJ/PjZ9OJ9esVivjnU7HNbVazXiSJK7JZrPGx+NxnIkQIhDRdmS5XBo/n8+uGQ6HxovFomuez6fxw+HgmuTF3oSZCCECIQIRbdmv16vxbrfrml6v9+kL8p3ZbGY8n8+7plwuI+xECBGIaDsyGAyMt1ot1ywWC+OTycQ1lUrFeKPRcE2hUEDYiRAiECIQ0Za9Wq0a3+/3rplOp8bX67VrRqOR8Xq97ppSqYSwEyFEIKLtyHa7/fLP7ng8Gm82m/7AnD3ydru55vF4GO/3+3EmQohAiEBEW/b7/W781dXjxy/kdrvtms1mY3y327nmcrkg7EQIEQgRMv/37H8MQgRCBEIEQgRCBEIEQgT+9gP8FG9fCleM1xj80AAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAWVJREFUeJztmTGKwmAUhMfJFkFEQUux8gKCV1AQxEZIKd4lrZWl2KT1GIKHsDB4gDQKFoJFli1fXhGLhd2df79uwgcJTIYfkkZZliUEIEQgRCBEIEQgRCBEIEQgRPh4V9xutyYXReGcPM9NXq/Xzun1eiafz2fnxHFscpIk4TRCiECEtpG88v7f73fnTCYTk2ezmXP2+73Jr9er9l7J/0b+IIQIRGhjj6Kodsjz+dzkLMucczweTR4MBs4ZDocIthFCBCK0jXQ6HZMXi0Xtjna7nXOqB+Dj8XDO8/lEsI0QIhAiEKGNvdvtmrzZbJyTpqnJo9HIOdVr4/HYOc1mE8E2QohAhLaRVqtl8ul0ck6/3zd5Op06p3qtur0vLpcLgm2EEIEQgRCh8e4/xOVyWXvYtdttk1erVe3noMPh4Jzb7Wby9XoNpxFCBCK0jfx2CBEIEQgRCBEIEQgRCBEIEfjTD/BdfAKAf1JnkauDFwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAWtJREFUeJztmS+KgnEURa/XP8n4LcEkJotiMdgEV+A2xCCC0abuwQ0YLSJYBLNVjBbRIAYt3+Q3jxknDMxwn6cdOOEHz4uCmTRNUwhAiECIQIhAiECIQIhAiECIkPtpOJ1Oja9WK9c8Hg/jzWbTNZfLxXi1WnXN4XAwPhqN4lyEEIGItpHFYmH8+Xy6ptvtGk+SxDXX69X4ZrNxzfl8RtiLECIQIhDRxn6/343X63XXtFot4/P53DXr9do+IOef0Ol0EPYihAhEtI0MBoOXn+3b7Wb8dDq5plQqfbu9r75sw1yEEIEQgYg29uTTL9nlcuma8XhsvFKpuKbdbhvv9/uuOR6PCHsRQgQi2ka2263xQqHgmkajYbxWq7mm1+sZz+fzrikWiwh7EUIEQgQi2tiz2azx/X7vmslk8nLsu93O+Gw2c025XDY+HA7jXIQQgRAh8/6f/Z9BiECIQIhAiECIQIhAiMC/fsBv8QE9ilWsV8lGvwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAWNJREFUeJztmT2KwmAYhMfBRlBM5QUsxNrCwhuYKkWwsPNInkOwU7CzsYkHUGyElMEu+FNFtnx9YQVZ2GW+fbonTEJgGJKQWlVVFQQgRCBEIEQgRCBEIEQgRCBEqH964mQycceKojA+m81cZrFYGB+Pxy6zXq+NL5fLcBohRCBC28hutzMeRZHLrFYr46PRyGU6nY7x0+nkMofDAcE2QohAiECENvbr9Wp8u926zHA4fDvay+ViPE1Tl+l2uwi2EUIEIrSNzOdz48fj0WVut5vxOI5dJkkS4/v93mU2mw2CbYQQgRCBCG3sxcvXX73uTx0MBsabzabLvI47z/O3b8hBNUKIQIS2kXa7bbzX67lMq9V6+9A8n8/Gp9Opy2RZhmAbIUQgRCBCG3tZlsYbjYbLPB4P4/1+32Xu9/u31/3ik9+aMo0QIhAi1P7/s/8xCBEIEQgRCBEIEQgRCBH42zfwUzwBtXZagw9aancAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAW9JREFUeJzlmSGOwlAYhIfJcoEaDEkVYEGCqcU2QeAIDkG4ADdAITkAFs0BSEpCMIiKOgSCILGIrv75ky1ik93M+9wkn2g6mfTltVaWZQkBCBEIEQgRCBEIEQgRCBEIEb4+FbvdrsmDwcA5h8PB5Far5RzSvrvxeOyc+/1u8nw+D6cRQgQitI2Ub2fL/X7vnGazafJkMnFOlmWVO3o8Hgi2EUIEQgQitLF3Oh2Ti6Jwzvtwt9utc9I0Nbndbjvncrkg2EYIEYjQNtLv901+Pp/OOR6PJi8WC+ecTieToyhyzu12Q7CNECIQIhChjT3P88pBNhoNk2ezmXPW67XJw+HQOefzGcE2QohAiFD79LfCbrczeTQaOWe1Wpkcx3HlDcnr9XJO9PaRnE6n4TRCiECIQIQ29l6vZ3KSJM6p1+uV10HL5dLkzWbjnOv1+uPJW7oRQgQitI38dwgRCBEIEQgRCBEIEQgRCBH41w/wW3wDxtFblDxAXCUAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAW5JREFUeJztmTGKwlAURa8XG23sLCwsxFKw0xQWiuAO3Iu4C1fgLqy1FbGwC1hZBRKIoIiNxHJ4vgEtBma4f0534MAPeTyFn1JRFAUEIEQgRCBEIEQgRCBEIEQgRCh/Gi6XS+OLxcI1zWbT+HA4dM3r/+9oNHLN7XYzPhgMwpkIIQIR2o7keW78fr+7plarGU+SxDXVatX4+Xx2zeVyQbATIUQgRCBCW/Ysy4zHceyayWRifDqduma327390TidTgh2IoQIRGg7kqap8U6n45per2ec9O/p8XgY7/f7rqnX658+1tdZEIEQgRCBCG3Zoygyvt/vXdNut40fj0d/YNkeuVqtXNNoNBDsRAgRiNB2ZLPZGO92u65Zr9fGZ7OZa+bz+dsbku12a3w8HoczEUIEQgQitGVvtVrGD4eDa14X97urntfroOv16ppKpYJgJ0KIQIhQ+v/O/scgRCBEIEQgRCBEIEQgROBvP8BP8QQ37VdrBmMnFgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAWBJREFUeJztmbGKwlAQRa+XoKSzEDsVCyUiQcHKXmzsBL9XEMHGMqWFIEiIhYWFhUWWLWeH1S0WdrnP0x04CYFheA9SKcuyhACECIQIhAiECIQIhAiECIQI0U/DbrdrfDAYuKbf7xtPkuTle4bDoWt2u53x1WoVzkQIEYjQdiSKbHq5XFyTpqnx5XLpmmazaXy73bomjmMEOxFCBEIEIrRlPxwOxheLhWvO57Px2+3mms1m8/SZ7w7JYCZCiECEtiMkn14QP6lWq8bX67VrRqOR8Xa77Zr4fSAKQIhAhLbsrVbLeJZlrpnNZsbzPHfN4/EwXhSFa+bzOYKdCCECEdqOjMdj47VazTWn0+nl5e/rYdfpdFxzPB6NT6fTcCZCiECIQIS27L1ez/j1enVNo9Ewvt/vXTOZTIzf73fX1Ot1BDsRQgRChMr7P/s/gxCBEIEQgRCBEIEQgRCBf/0Bv8UHrRNFkoesiX4AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXBJREFUeJzlmT2KwmAURa+XIV06K9FCIYV29m7BTnfgFtyJS7CyThewsgqSxkZBQQULwVIQQcgw5fMNo8XADPc73YFTBF4eX34qZVmWEIAQgRCBEIEQgRCBEIEQgRDh491wOp0aX61WrpnP58b7/b5rdrud8SiKXLPf741nWRbORAgRiNB2JHq6l2u1mmtarZbxTqfjmu12a3yxWLhmvV4j2IkQIhAiEKEt+2azMZ4kiWvG47Hx4/Homm63a/x6vbom+uaQDGYihAhEaDtyOByMDwYD15xOJ+OXy8U1RVG8fCAcjUYIdiKECIQIRGjLnjwdgO122zWz2cz47XZzzXK5NN7r9VxTr9cR7EQIEYjQdqRarRp/PB6uaTabxtM0dc1wOPzxjfGLPM8R7EQIEQgRCBEq7/5DPJ/PLxf5+S1yMpm4Jo5j441GwzX3+/3l51nZiRAiEKHtyH+HEIEQgRCBEIEQgRCBEIEQgX99Ab/FJ0AmYUC6A6Y8AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAV9JREFUeJzt2bGKwmAQBOBxFGwUUtjYSCqthIBd3sCH9BmsBYsUvoBWdppgF0RRRLscVsfeysXi4I757+tGJvKHzZIijaqqKgggRBAiCBGECEIEIYIQQYhovVucz+cmX69X17ndbia/etdmWWZyu912ndVqZfJutwtnIoQIIrQdud/v9sKWv/Trb6+e7SRJTF4ul64znU4R7EQIEYQIIrRlHwwGJsdx7Dr7/d7kx+PhOrPZzOT1eu068Yv/DmYihAgitB1pNpvf5qeyLE3udruu0+v1TB4Oh65zOBwQ7EQIEYQIIrRlP5/PJvf7/doX2WKxcJ08z02+XC6uM5lMEOxECBFEaDtStzNP2+3W5KIoXKfT6Zg8Go1cJ01TBDsRQgQhgght2Ul7z5vNpnbZj8ej60RRZPJ4PHad0+n07rE+zwcRhAhCROP/O/sfQ4ggRBAiCBGECEIEIYK/fYCf8gHeZlUu9uLD5AAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXxJREFUeJztlqGKAlEUhn9/VycIFsEm2LQKJhURg+BDWMRiMJhslslis/sCRpvgC5gNRqugPsEsG49n2V2WBXf/u1/7hm9gmMPh3lSSJAkEIEQgRCBEIEQgRCBEIEQgRHj57ouXy8U9G41GxovFomvy+bzxxWLhmt1uZ7zb7YYzEUIEQoTUVy+Nk8nEeK/Xc02tVjN+OBxcs91ujRcKBdeUSiXj4/E4nIkQIhAiEKEdiJlMxvhqtXJNHMfG0+m0ayqVivHlcumaZrNp/H/Z/yJEaAdiNpv9cB/euN1un14sHw9N0v/L+/1ufDabhTMRQgRCBCK0Ze/3+8bP57Nr2u228Uaj4ZrNZmN8OBy6ZjqdGj+dTuFMhBCBCO3S2Ol0jL+3Wrlczvj1enVNtVo1Xi6XXTOfzxHsRAgRCBGI0JY9eVjuVqvlmv1+b7xer7vmeDwaj6LINev12vhgMAhnIoQIRGiXxt8OIQIhAiECIQIhAiECIQIhAp/9AT/FKyqcWuMjSqvmAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXlJREFUeJzll6GqAlEURfdsXlAQRNCoxaJhQPAPFJsgFo1q8xv8AH/BOlhMJkHwFwx+wiRBs0kUnJfPO0HDg/fYd7UFC+7A4XLuRFmWZRCAEIEQgRCBEIEQgRCBEIEQ4evTMEkS4+Vy2TXX69X4fr93zXg8Nl6pVN6etV6vw5kIIQIR2h1pNBrGc7mcazqdjvHj8eia5XJpPI5j19RqNQQ7EUIEQgRChOjTP8Qoioxvt1vXPB4P471ezzWLxcJ4q9VyzW63e7tYZSdCiECEthBHo5HxNE1dczqdjBeLRdc0m03j9/vdNa/XC8FOhBCBEIEIbSEeDgfjs9nMNfP53Pjz+XRNtVo1Xq/XXXO73YwPh8NwJkKIQIS2EC+Xi/HBYOCazWZjvN1uu6ZQKBhfrVau6ff7CHYihAiECERolz398drN5/Ou6Xa7xieTiWtKpZLx8/nsmul0imAnQohAhPZo/O8QIhAiECIQIhAiECIQIhAi8K8/4Lf4BkdNWKkvJgV1AAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXZJREFUeJzlmDGKwlAURW8u04oiiFbpXIFilybgEqzcgoWgK3AR9mIhuAorwSqFhbVaCWoRtM5g+XziWAzMcP/pDpwi8Eje/4mKoiggACECIQIhAiECIQIhAiECIcLXp+F2uzUeRZFrJpOJ8dvt5po8z42naeqa8/lsfDqdhjMRQgRChOjTQ+NutzPebDZds9/vjS8WC9dsNhvjtVrNNVmWvXXpiRAiECIQoS3Ew+FgfD6fu6ZUKhlvtVqu6Xa7xgeDgWv6/T6CnQghAhHaO7Jer43Hceyacrn847Jrt9vGR6ORa2azmfHxeBzORAgRCBGI0E6/6dNNLkkS16xWK+PD4dA19/v97Y3x1Uei1+shmIkQIhChLcRKpWL8eDy6ptFovF1sDzqdjvHT6eSa6/WKYCdCiECIQIT2ssdPp93n30MP6vW68eVy6ZpqtWr8crm4Jn+xJIOZCCECEdqh8b9DiECIQIhAiECIQIhAiECIwL9+gN/iGxr1YTpW6GMjAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXhJREFUeJzlmK2qAmEURbebsWgxDBPsWi2CTbDYNSoWH8L38AUsJqsMvoFYDQpqUfAPqy8wxsvxBA0X7mV/qy1YDMMczvzlsizLIAAhAiECIQIhAiECIQIhAiFC9G14Op2M73Y718znc+NJkrimVqsZb7VarikWi8bz+Xw4EyFEIELbkdvtZvz5fH5sDoeDa8rlsvH1eu2a8/lsfDAYhDMRQgRCBEKE3LdfiMfj0Xgcx64ZjUbGo8jfS7bbrfFCoeCaNE0R7EQIEYjQHojL5dJ4o9FwzWq1Mt7v911zvV6Nt9tt1wyHQ+OTySSciRAiECIQoS178va1t9/vXdPpdIyPx+OPy076a9nr9b49rZ/jQARCBCK0HalWq8Yvl4trFouF8Waz6ZpKpWL8fr+7ZrPZGO92u+FMhBCBEIEIbdlns5nxx+Phmve33el06pr3B2m9XndNqVRCsBMhRCBC+4vy3yFEIEQgRCBEIEQgRCBEIETgX5/Ab/ECjjZdUNX4dYYAAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAW1JREFUeJzlmDGKAlEQBZ+PFRQMxEAzwUQZMDUw9QSewVxvYeoFjLyJZmYmnkATUUQDUUFhFsO2YcdgYZfXlRVUMND86T+TS9M0hQCECIQIhAiECIQIhAiECIQIX5+G3W7XeL1ed831ejXe6XRcc7lcjD+fT9e0223jg8EgzkQIEYhoZ2S5XBpvNBquOZ/PxiuVimvm87nx2+3mmlqthrATIUQgRCBEyH36hTgajYzvdjvX9Pt94/f73TXVatX4YrHIfGlMp9M4EyFEIKItxCRJfrz8vSgUCsZXq5VrSqWS8fV67ZrT6YSwEyFEIEQgoi3E5O2wvy+/F61Wy/h2u3XNfr/P/EIsFovGJ5NJnIkQIhDRFmKz2TSez+cz/6IMh0PXlMtl47PZzDWHwwFhJ0KIQIhARDvsvV7P+OPxyFxkm83GNePx2PjxeMx8IYSaCCECEe3S+N8hRCBEIEQgRCBEIEQgRCBE4F8/wG/xDQMWYkun4MSFAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAW5JREFUeJzlmaGqAlEURbd7HgaLyaJmg+gEg2EQDP6C+CN+gZ/hD5iNisUJWgyWiZMNgjabOC8fz4MxPHiPfVdbsMKFw+EydypFURQQgBCBEIEQgRCBEIEQgRCBEOHr03Cz2RjP89w1tVrN+O12c81utzPe6/Vc83g8jC+Xy3AmQohAhLYj8/nc+GAwcM14PDZ+Pp9dM51OjT+fz9JdC2oihAiECERoy75YLEovu9FoZLzb7brmdDoZbzQarnm9Xgh2IoQIRGg70u/3jR+PR9c0m03j7XbbNUmSGL/f7665Xq8IdiKECIQIRGjLnqap8U6n45r1em28Xq+75nA4GJ/NZq6pVqsIdiKECERoO7Lf741nWeaaKIqMTyaT0ot1tVq5ptVqGY/jOJyJECIQIhAiVD79h/h+Sb1/Df70RLTdbl1zuVyMD4fD0ufZ4oMjykyEEIEIbUf+O4QIhAiECIQIhAiECIQIhAj86wP8Ft9irFaHzV+SkgAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAXFJREFUeJztmDGKwlAURa8XKxEr7cRCLMTGLMLC0lYR3IBLcC3iKiwEdQdGG3eghaSRgAgqmfr5BrQYmJn7Pd2BEwi8PJL8XJZlGQQgRCBEIEQgRCBEIEQgRCBEyL8bbjYb45VKxTWLxcJ4kiSumU6nxs/ns2smk4nx8XgczkQIEYjQdmQ+n798tmezmfHH4+GaTqdjvFqtuqbf7yPYiRAiECIQoS37arUyvl6vXdNsNo13u13XjEYj41EUueZ2uyHYiRAiECLk3j1FGQ6HxovFomued6LVarkmn7dreTweXZM8fWz2er1wJkKIQIhAhPZCvFwuxuv1umtOp5PxcrnsmuVyabxWq7lmt9sZ/yz7f4QIbUcajYbxwWDgmjiOjV+vV9e0223jpVLJNdvtFsFOhBCBEIEIbdkLhYLx/X7vmvv9bjxNU9ccDoeXR0bpN9cFMxFCBCK0P8S/DiECIQIhAiECIQIhAiECIQJ/+wZ+ii+TJmHWN3sXAQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAWpJREFUeJztmb1qAmEQRa+XgJYWCloIFhYiq2BlaeUTWGrlE/hAvoGFb2CpLIIWFv5gZWNpL8KGlJOBCCGQcCenO8v5YNlhWNjNZVmWQQBCBEIEQgRCBEIEQgRCBEKEt+8e3O/37tpisTBeKpVenlsul64pFovG0zSNMxFCBCLajkwmE+OVSsU18/nc+OVycU25XDbe7/ddMxwOEXYihAiECES0ZT8cDsbX67Vr6vW68W6365rpdGr8+Xy65nq9IuxECBGIaDvSbreNk/4ZtFot44PBwDWbzcZ4oVBwzWq1Mj4ej+NMhBCBEIGItuyNRsP44/FwTT6fN34+n13z+QU4m81cU61WEXYihAhEtB05Ho/Ge72ea5rNpvHdbuea2+325e590Ol0EHYihAiECES0ZR+NRsZPp5Nrttut8fv97pparfbyk2mSJAg7EUIEQoTc/3/2PwYhAiECIQIhAiECIQIhAn/7Bn6Kd8+OUYZj2wFoAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADIAAAAyCAYAAAAeP4ixAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAAW1JREFUeJztma2KQmEURbebMRj8QeU2w71iMAgmwUdRfAerT+QLmLUYvckgWASbCgYtGoQ7GI8HxgkDM+xvVluwgrA58F3MZVmWQQBCBEIEQgRCBEIEQgRCBEKEj++Gp9PJeBRFrplOp8Zns5lr0jQ13mw2XXM+n42vVqtwFiFEIEK7keFwaDyfz7tmvV5/eVdPHo+H8dFo5JrD4YBgFyFEIEQgQjv2/X5v/Hg8uqbVahlvt9uuGQwGxjudjmsWiwWCXYQQgQjtRpIkeftoLBaLxuv1umtut5vx8XjsmkqlYnwymYSzCCECIQIR2rHXajXj/X7/7cv29WvwyXK5NN7r9Vyz3W4R7CKECERoN1IqlYzP53PXdLtd44VCwTWNRsN4tVp1zf1+R7CLECIQIhChHXv08tq9Xq+uuVwuxuM4ds1utzO+2WxcUy6XEewihAiECLn//9n/GIQIhAiECIQIhAiECIQI/O0f8FN8AnkAUy0iGUGqAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 40x40 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize weights as images\n",
        "def imshow(img):\n",
        "    plt.figure(figsize=(0.4, 0.4))\n",
        "    plt.imshow(img, cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Get the weights\n",
        "for name, param in net.named_parameters():\n",
        "    if 'weight' in name:\n",
        "        weights = param.data.cpu()\n",
        "        # Visualize the weights as images\n",
        "        if len(weights.size()) == 4:  # Convolutional layer\n",
        "            print(name)\n",
        "            for i in range(weights.size(0)):\n",
        "                for j in range(weights.size(1)):\n",
        "                  imshow(weights[i,j])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0_AEkXpmXpk"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5dgJ3hAtE7Z"
      },
      "source": [
        "## Let's play with some common pretrained deep nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04DzkLKq1EaE"
      },
      "source": [
        "Resnets are a class of famous conv nets that worked accurately on a range os classification tasks.\n",
        "\n",
        "The key idea in resnet's was residual connections that enabled training of very deep networks.\n",
        "\n",
        "We shall see how the deeper a network, the better the accuracy at the cost of computation.\n",
        "\n",
        "A common theme in the 2016-18 times was to use pretrained features from Renset for various custom tasks on custom datasets.\n",
        "\n",
        "Your challenge would be to extract the last layer features from the resnet and train it on CIFAR 10 to improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JMnf8_6zCKs"
      },
      "source": [
        "### Pretrained models are trained with a specific image preprocessing.\n",
        "\n",
        "Hence, we must take care to preprocess the image the same way.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CCtbIju2pGMF",
        "outputId": "81a8d050-663d-434d-91f7-b72412df9cd8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/spectual/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ImageClassification(\n",
            "    crop_size=[224]\n",
            "    resize_size=[256]\n",
            "    mean=[0.485, 0.456, 0.406]\n",
            "    std=[0.229, 0.224, 0.225]\n",
            "    interpolation=InterpolationMode.BILINEAR\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "weights = ResNet18_Weights.DEFAULT\n",
        "model = resnet18(weights=weights)\n",
        "model.eval()\n",
        "# Initialize the inference transforms\n",
        "preprocess = weights.transforms()\n",
        "print(preprocess)\n",
        "# Apply inference preprocessing transforms to the dataloaders\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                       download=True, transform=preprocess)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=preprocess)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAhtHTyN4Qyc"
      },
      "source": [
        "### However, there is a challenge.\n",
        "\n",
        "We cannot apply this network directly to CIFAR 10 for instance, since the output of the pretrained network is 1000 classes (for imagenet), and we only have 10 classes in CIFAR 10.\n",
        "\n",
        "Hence, we need to adapt it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmNLNg9K2YO8"
      },
      "source": [
        "## Challenge 1: Extract the last feature and train a net on top on CIFAR 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ouUDSVvQZD9K",
        "outputId": "810c0a29-5092-460a-d8dc-08528eb00c4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IHsg63jdpGMG",
        "outputId": "77b0210f-4db0-4779-997e-0821232b18f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchsummary\n",
            "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
            "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
            "Installing collected packages: torchsummary\n",
            "Successfully installed torchsummary-1.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lfmkMftxYaWt",
        "outputId": "c5b31077-f426-4afc-9159-739e47d301d9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/spectual/miniconda3/envs/torch311/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/spectual/miniconda3/envs/torch311/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                 [-1, 1000]         513,000\n",
            "================================================================\n",
            "Total params: 11,689,512\n",
            "Trainable params: 11,689,512\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 44.59\n",
            "Estimated Total Size (MB): 107.96\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "model1 = models.resnet18(pretrained=True).to(device)\n",
        "#model1.eval()\n",
        "summary(model1, input_size=(3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1ZqnLQn-2kmD",
        "outputId": "85f9da87-a76d-482e-9053-16733d924ba0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2001it [03:00, 11.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 0.716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4001it [05:50, 11.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  4000] loss: 0.545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6002it [08:45, 11.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  6000] loss: 0.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8001it [11:46, 11.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  8000] loss: 0.509\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10002it [14:34, 11.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 10000] loss: 0.508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12001it [17:37, 11.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 12000] loss: 0.473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12500it [18:37, 11.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 84.52%\n",
            "Avg time taken for prediction: 0.09350609922409057\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "device = torch.device('mps' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "newmodel = models.resnet18(pretrained=True).to(device)\n",
        "newmodel.eval()\n",
        "\n",
        "# step 1: make a new network that will replace the resnet head, you can add more layers if you like and compare the results\n",
        "out_dim = 512\n",
        "class MyNewResnet(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(MyNewResnet, self).__init__()\n",
        "        #Create a linear layer with the input being the output dimension size and the output size being the number of classes in CIFAR-10\n",
        "        self.fc3 = nn.Linear(out_dim, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "new_resnet = MyNewResnet().to(device)\n",
        "#This replaces the Resnet classification head with our fully connected layer\n",
        "newmodel.fc = new_resnet\n",
        "\n",
        "#Here we freeze the previous layers in ResNet and only update the parameters in the final fully connected layer\n",
        "for param in newmodel.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in newmodel.fc.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(newmodel.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# step 3: train this new network on CIFAR 10\n",
        "# Training the network\n",
        "for epoch in range(1):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in tqdm.tqdm(enumerate(trainloader, 0)):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = newmodel(inputs.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "\n",
        "# step 4: test the accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "avg_time = []\n",
        "for data in testloader:\n",
        "    images, labels = data\n",
        "    start = time.time()\n",
        "    #outputs = newmodel(images.cuda())\n",
        "    outputs = newmodel(images.to(device))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    end = time.time()\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.cpu() == labels).sum().item()\n",
        "    time_taken = end - start\n",
        "    avg_time.append(time_taken)\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total}%\")\n",
        "print(f\"Avg time taken for prediction: {np.average(avg_time)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E34q79Ue2FnV"
      },
      "source": [
        "# Here we are simply increasing the number of epochs to see if it improves performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "IV8FP6af13QB",
        "outputId": "b2883a10-b466-493f-fc81-ecb68fd9824c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/spectual/miniconda3/envs/torch311/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/Users/spectual/miniconda3/envs/torch311/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "2011it [00:39, 57.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  2000] loss: 0.716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4010it [01:12, 60.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  4000] loss: 0.546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6012it [01:46, 59.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  6000] loss: 0.518\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8010it [02:20, 59.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,  8000] loss: 0.508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10006it [02:53, 59.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 10000] loss: 0.508\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12007it [03:27, 59.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1, 12000] loss: 0.472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12500it [03:45, 55.43it/s]\n",
            "2007it [00:33, 60.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2,  2000] loss: 0.454\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4010it [01:07, 59.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2,  4000] loss: 0.470\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6012it [01:40, 60.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2,  6000] loss: 0.463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8009it [02:14, 59.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2,  8000] loss: 0.467\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10007it [02:48, 60.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 10000] loss: 0.472\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12006it [03:21, 61.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2, 12000] loss: 0.442\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12500it [03:39, 56.94it/s]\n",
            "2011it [00:32, 60.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3,  2000] loss: 0.432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "4012it [01:05, 60.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3,  4000] loss: 0.452\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "6008it [01:38, 61.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3,  6000] loss: 0.446\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "8012it [02:11, 61.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3,  8000] loss: 0.451\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "10010it [02:44, 59.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 10000] loss: 0.457\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12011it [03:17, 60.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3, 12000] loss: 0.427\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "12500it [03:36, 57.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the 10000 test images: 85.45%\n",
            "Avg time taken for prediction: 0.007691251564025879\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "device = torch.device('mps')\n",
        "\n",
        "newmodel = models.resnet18(pretrained=True).to(device)\n",
        "newmodel.eval()\n",
        "\n",
        "# step 1: make a new network that will replace the resnet head, you can add more layers if you like and compare the results\n",
        "out_dim = 512\n",
        "class MyNewResnet(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(MyNewResnet, self).__init__()\n",
        "\n",
        "        #Create a linear layer with the input being the output dimension size and the output size being the number of classes in CIFAR-10\n",
        "        self.fc3 = nn.Linear(out_dim, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.fc3(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "new_resnet = MyNewResnet().to(device)\n",
        "#This replaces the Resnet classification head with our fully connected layer\n",
        "newmodel.fc = new_resnet\n",
        "#print(new_resnet)\n",
        "for param in newmodel.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in newmodel.fc.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(newmodel.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# step 3: train this new network on CIFAR 10\n",
        "# Training the network\n",
        "for epoch in range(3):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in tqdm.tqdm(enumerate(trainloader, 0)):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = newmodel(inputs.to(device))\n",
        "        loss = criterion(outputs, labels.to(device))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "\n",
        "# step 4: test the accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "avg_time = []\n",
        "for data in testloader:\n",
        "    images, labels = data\n",
        "    start = time.time()\n",
        "    outputs = newmodel(images.to(device))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    end = time.time()\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted.cpu() == labels).sum().item()\n",
        "    time_taken = end - start\n",
        "    avg_time.append(time_taken)\n",
        "\n",
        "print(f\"Accuracy of the network on the 10000 test images: {100 * correct / total}%\")\n",
        "print(f\"Avg time taken for prediction: {np.average(avg_time)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjjZ_nB_Uyco"
      },
      "source": [
        "### Challenge : Repeat the same for ResNet 50 and observe the change in accuracy! :p"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn60KnCWr6nL"
      },
      "source": [
        "### Misc Notes:\n",
        "\n",
        "Fore more state-of-the art models, refer to Huggingface. https://huggingface.co/models\n",
        "\n",
        "https://www.cs.cmu.edu/~epxing/Class/10708-19/notes/lecture-16/ - CNN section.\n",
        "https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf - NeurIPS paper that showed feature hierarchy in CNN's.\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
